# -*- coding: utf-8 -*-
"""major model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SOM9A6qacR5voaFUiwpXJqvQ-VqDFERC
"""

import pandas as pd

df=pd.read_csv('/content/drive/My Drive/major1/dataset/IMDB Dataset.csv.zip')
df

import re
def  clean_text(df, text_field, new_text_field_name):
    df[new_text_field_name] = df['review'].str.lower()
    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r"(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|^rt|http.+?", "", elem))  
    # remove numbers
    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r"\d+", "", elem))
    
    return df
data_clean = clean_text(df, 'text', 'text_clean')
data_clean.head()

import nltk.corpus
nltk.download('stopwords')
from nltk.corpus import stopwords
stop = stopwords.words('english')
data_clean['text_clean'] = data_clean['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
data_clean.head()

from nltk.stem import PorterStemmer 
from nltk.tokenize import word_tokenize

import nltk 
nltk.download('punkt')
from nltk.tokenize import sent_tokenize, word_tokenize
data_clean['text_tokens'] = data_clean['text_clean'].apply(lambda x: word_tokenize(x))


data_clean.head()









def word_stemmer(text):
    stem_text = [PorterStemmer().stem(i) for i in text]
    return stem_text
data_clean['text_tokens_stem'] = data_clean['text_tokens'].apply(lambda x: word_stemmer(x))
data_clean.head()

nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
def word_lemmatizer(text):
    lem_text = [WordNetLemmatizer().lemmatize(i) for i in text]
    return lem_text
data_clean['text_tokens_lemma'] = data_clean['text_tokens'].apply(lambda x: word_lemmatizer(x))
data_clean.head()









x = data_clean.iloc[:,2].values
y = data_clean.iloc[:,1].values

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=0)

x_train.shape

x_test.shape

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
#Pipeline([('Variable1',Method1()),('Variable2',Method2())])

text_model = Pipeline([('tfidf',TfidfVectorizer()),('model',MultinomialNB())])







text_model.fit(x_train,y_train)

y_pred = text_model.predict(x_test)
y_pred

y_test

# Evaluation: Accuracy Score, Confusion Matrix
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
accuracy_score(y_pred,y_test)

print(accuracy_score(y_test,y_pred)*100)
print(classification_report(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))



# Commented out IPython magic to ensure Python compatibility.
# %%writefile app1.py
# import streamlit as st
# import pandas as pd
# import numpy as np
# from sklearn.pipeline import Pipeline
# from sklearn.naive_bayes import MultinomialNB
# from sklearn.feature_extraction.text import TfidfVectorizer
# import pandas as pd
# 
# df=pd.read_csv('/content/drive/My Drive/major1/dataset/IMDB Dataset.csv.zip')
# import re
# def  clean_text(df, text_field, new_text_field_name):
#     df[new_text_field_name] = df['review'].str.lower()
#     df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r"(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|^rt|http.+?", "", elem))  
#     # remove numbers
#     df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r"\d+", "", elem))
#     
#     return df
# data_clean = clean_text(df, 'text', 'text_clean')
# import nltk.corpus
# nltk.download('stopwords')
# from nltk.corpus import stopwords
# stop = stopwords.words('english')
# data_clean['text_clean'] = data_clean['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
# x = data_clean.iloc[:,2].values
# y = data_clean.iloc[:,1].values
# 
# st.title("Movie Review Sentiments")
# st.subheader('TFIDF Vectorizer')
# st.write('This Project is based on Naive bayes')
# text_model=Pipeline([('tfidf',TfidfVectorizer()),('model',MultinomialNB())])
# text_model.fit(x,y)
# review=st.text_area("Enter Text","Type Here...")
# op=text_model.predict([review])
# if st.button("Predict"):
#   st.title(op)

!pip install streamlit

!streamlit run app1.py



!pip install pyngrok

from pyngrok import ngrok
url=ngrok.connect(port='8501')
url

